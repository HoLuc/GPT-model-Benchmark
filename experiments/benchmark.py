import re
import numpy as np
import pandas as pd
from bleurt import score as Bleurscore
from rouge import Rouge
from nltk.translate.bleu_score import sentence_bleu as BLEUscore
import warnings


# Ignore possible warnings from librairies
warnings.filterwarnings('ignore')


def load_test_data(path='data/test.csv'):
    """Loading of the reference sentence for the summarization task

    Args:
        path (str, optional): Path of the test dataset. Defaults to 'data/test.csv'.

    Returns:
        list_highlights (list): List of reference sentence
    """
    database = pd.read_csv(path)
    test_data = database.sample(frac=0.01, replace=True, random_state=1)
    list_highlights = test_data["highlights"].tolist()
    return list_highlights

def data_loading_and_cleaning(path='results/generated_summary.txt'):
    """Loading and Cleaning of the sentences generated by the models

    Args:
        path (str, optional): path where the . Defaults to 'results/generated_summary.txt'.

    Returns:
        summary_list (list): List of the cleaned sentences
    """
    #Results Preprocessing/Cleaning
    summary_raw = pd.read_table(path).values.tolist()

    summary_list = []
    for i in range(len(summary_raw)):
        summary_set = re.split('", "', summary_raw[i][0], maxsplit=2)
        if len(summary_set) == 3:
            summary_list.append(summary_set)
        else:
            summary_set = re.split('", "|",', summary_raw[i][0], maxsplit=2)
            if len(summary_set) == 3:
                summary_list.append(summary_set)
            else:
                summary_set = re.split("',", summary_raw[i][0], maxsplit=2)
                if len(summary_set) == 3:
                    summary_list.append(summary_set)
                else:
                    summary_raw[i][0]  =  summary_raw[i][0].replace('",', "',")
                    summary_set = re.split("', ", summary_raw[i][0], maxsplit=2)
                    summary_list.append(summary_set)

    for i in range(len(summary_list)):
        for j in range(len(summary_list[i])):
            summary_list[i][j] = summary_list[i][j]\
                .replace('["', '')\
                .replace('\\n', '')\
                .replace('"]', '')\
                .replace("['", '')\
                .replace("]", '')
    return summary_list

def average_rouge_f1score(model_summary, list_highlights, n_gram):
    """Compute the Average F1-score of ROUGE metric value over the sentences generated by the model

    Args:
        model_summary (list): List of summaries generated by the selected model
        list_highlights (list): List of reference sentence
        n_gram (int or str): n_gram selected to compute ROUGE metric

    Returns:
        average_f1score (float): Average F1-score of ROUGE metric value
    """
    assert n_gram in [1, 2, 'l'], 'n_gram has be 1, 2 or l'
    
    ROUGE = Rouge()
    model_rouge_score = ROUGE.get_scores(model_summary, list_highlights) 
    average_f1score = sum(d[f'rouge-{str(n_gram)}']['f'] for d in model_rouge_score) / len(model_rouge_score)
    return average_f1score

def average_bleu_score(model_summary, list_highlights):
    """Compute the Average BLEU metric value over the sentences generated by the model

    Args:
        model_summary (list): List of summaries generated by the selected model
        list_highlights (list): List of reference sentence

    Returns:
        ndarray: Average BLEU metric value
    """
    list_bleu_score = [BLEUscore(list_highlights[i], model_summary[i]) for i in range(len(model_summary))]
    return np.mean(list_bleu_score)

def average_bleurt_score(model_summary, list_highlights):
    """Compute the Average BLEURT metric value over the sentences generated by the model

    Args:
        model_summary (list): List of summaries generated by the selected model
        list_highlights (list): List of reference sentence

    Returns:
         ndarray: Average BLEURT metric value
    """
    scorer = Bleurscore.BleurtScorer()
    list_bleurt_score = [scorer.score(references=[list_highlights[i]], candidates=[model_summary[i]]) for i in range(len(model_summary))]
    return np.mean(list_bleurt_score)
    

if __name__ == '__main__':
    # Loading of the test dataset
    list_highlights = load_test_data()
    # Loading and Cleaning of the generated sentences
    summary_list = data_loading_and_cleaning()
    bert_summary = [row[0] for row in summary_list]
    gpt2_summary = [row[1] for row in summary_list]
    davinci_summary = [row[2] for row in summary_list]
    
    # Average metric value computation
    results = []
    results.append(['Average Metric value', 'BERT', 'GPT-2', 'Davinci'])
    #Application of ROUGE metric
    results.append(['Rouge-1 F1-Score',\
                    average_rouge_f1score(bert_summary, list_highlights, n_gram=1),\
                    average_rouge_f1score(gpt2_summary, list_highlights, n_gram=1),\
                    average_rouge_f1score(davinci_summary, list_highlights, n_gram=1)])
    results.append(['Rouge-2 F1-Score',\
                    average_rouge_f1score(bert_summary, list_highlights, n_gram=2),\
                    average_rouge_f1score(gpt2_summary, list_highlights, n_gram=2),\
                    average_rouge_f1score(davinci_summary, list_highlights, n_gram=2)])
    results.append(['Rouge-l F1-Score',\
                    average_rouge_f1score(bert_summary, list_highlights, n_gram='l'),\
                    average_rouge_f1score(gpt2_summary, list_highlights, n_gram='l'),\
                    average_rouge_f1score(davinci_summary, list_highlights, n_gram='l')])

    #Application of BLEU metric
    results.append(['BLEU Score',\
                    average_bleu_score(bert_summary, list_highlights),\
                    average_bleu_score(gpt2_summary, list_highlights),\
                    average_bleu_score(davinci_summary, list_highlights)])

    #Application of BLEURT metric
    results.append(['BLEURT Score',\
                    average_bleurt_score(bert_summary, list_highlights),\
                    average_bleurt_score(gpt2_summary, list_highlights),\
                    average_bleurt_score(davinci_summary, list_highlights)])

    df_results = pd.DataFrame(results)
    df_results.to_csv(f'results/model_summarization_score.csv')
    print(df_results)
